# Dataset config
russian_language: False

# Common settings
random_state: 42

# Resources config
gpus: [7] # Please leave empty if you do not want to use any cuda devices
num_cpu: 42
batch_size: 256

# Model configs

# BigARTM parameters
artm_config:
   tiling_window_size: 5
   tiling_threshold: 0.6 # value between 0 and 1, recommended range 0.55-0.85
   smoothing_passes: 3
   smoothing_window: 3
   topics_qty: 50
   tokens_qty: 10
   sp_phi: -1
   sp_pheta: -0.5
   dec_phi: 100000
   collection_passes: 30
   min_dict_freq: 50

# sentence bert parameters for embeddings calculation
sbert:
   cpu_batch_size: 256
   gpu_batch_size: 256

# BERTopic parameters for segmentation
bertopic:
   do_train: false  # if true, it will train a model on the entire dataset
                    # if false, it will take text, train BERTopic on it, and then use topic vectors for segmentation
   verbose: false
   n_leave: null  # how many sentences to select from each document while training
   n_skip: 1  # will use each n_skip'th element for training. useful for small memory when doing umap and hdbscan
   reduce_outliers: true
   tiling:
      plot: false
      window_size: 4
      threshold: 0.5
      smoothing_passes: 1
      smoothing_window: 1
      savgol:
         n_smooth_savgol: 3
         polyorder: 3
   CountVectorizer:
      ngrams: 1  # 1 useful for unigrams for coherence calculations, 
        # but you can set it to 2 to consider bigrams too
      min_df: 2
      max_df: 0.95
   MaximalMarginalRelevance:
      diversity: 0.3
   pickle_path: null  # set null if you don't want to load/save the entire model (bertopic+umap+hdbscan)

# Summarization segmentation
sumseg:
   tiling:
      plot: false
      window_size: 5
      threshold: 0.7
      smoothing_passes: 1
      smoothing_window: 1
      savgol:
         n_smooth_savgol: 3
         polyorder: 3

# algorithms
cuml:
   use_cuml: true  # true for for corpus-level bertopic, false for document-level bertopic
   device: 'gpu'

# umap
umap:
   n_components: 5
   n_neighbors: 15  # 15 for for corpus-level bertopic, 5 for document-level bertopic
   min_dist: 0.0
   metric: 'hellinger'
   batch_size: 2048  # batch size for transforming data

# hdbscan
hdbscan:
   min_samples: 100  # set it to the smallest size grouping that you wish to consider a cluster
      # 100 for for corpus-level bertopic, 3 for document-level bertopic
   min_cluster_size: 3  # the larger the value of min_samples you provide, the more conservative the clustering â€“ 
      # more points will be declared as noise, and clusters will be restricted to progressively more dense areas.
      # 3 for for corpus-level bertopic, 1 for document-level bertopic

# tiling algorithm config
tiling_window_size: 5
tiling_threshold: 0.7 # value between 0 and 1, recommended range 0.55-0.85
smoothing_passes: 3
smoothing_window: 3

# used for argparser, don't touch it
subcommand: null
artm_subcommand: null
sbert_subcommand: null
input_path: null
output_path: null
collection: null
dictionary: null
model_path: null
predicts_path: null
clustering: null
dataset_type: null
train_path: null
test_path: null

# used for sbert embedding calculation, mainly for test, to don't wait for long calculations
sample_size: null

# dataset statistics used by random pipelines
mean_segment_length_wiki: 12.783  # train wiki727k
mean_segment_length_ami: 34.644   # full ami
