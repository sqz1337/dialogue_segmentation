{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cf51a86c-178f-47bd-9fbc-c4063d173177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "import contractions\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from pathlib2 import Path\n",
    "from datasets import Dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import artm\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_list(values):\n",
    "    plt.plot(values)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf282f86-733d-449d-9557-efb70f4d58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0dc55d0-e9b5-4806-a308-cc3478e1f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundaries(labels):\n",
    "    assert len(labels) > 1\n",
    "    boundaries = '0'\n",
    "    for i in range(1, len(labels)):\n",
    "        if labels[i] != labels[i-1]:\n",
    "            boundaries += '1'\n",
    "        else:\n",
    "            boundaries += '0'\n",
    "    return boundaries\n",
    "\n",
    "\n",
    "class WikiDataset:\n",
    "    def __init__(self, root):\n",
    "        self.textfiles = self._get_files(root)\n",
    "        self.separator = '========'\n",
    "\n",
    "    def _get_files(self, path):\n",
    "        'Ref: https://github.com/koomri/text-segmentation'\n",
    "        all_objects = Path(path).glob('**/*')\n",
    "        files = [str(p) for p in all_objects if p.is_file()]\n",
    "        return files\n",
    "\n",
    "    def _get_sections(self, lines):\n",
    "        '''Divide the text using separator on parts of text, where each part\n",
    "        has a different topic. Later we'll use it for a segmentation.\n",
    "        '''\n",
    "        sections = []\n",
    "        labels = []\n",
    "        last_is_sep = False\n",
    "        topic_id = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.replace('\\n', '')\n",
    "            if len(line):\n",
    "                if self.separator in line:\n",
    "                    if not last_is_sep:\n",
    "                        topic_id += 1\n",
    "                        last_is_sep = True\n",
    "                else:\n",
    "                    last_is_sep = False\n",
    "                    sections.append(line)\n",
    "                    labels.append(topic_id)\n",
    "                    \n",
    "        return sections, labels\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_sample(self):\n",
    "        for path in self.textfiles:\n",
    "            with open(path, 'r') as f:\n",
    "                lines = f.readlines()[1:]  # skip the first separator\n",
    "            if len(lines) < 1:\n",
    "                continue\n",
    "                \n",
    "            sections, labels = self._get_sections(lines)\n",
    "            if len(labels) <= 1:\n",
    "                continue\n",
    "                \n",
    "            boundaries = get_boundaries(labels)\n",
    "            yield {'path': str(path), \n",
    "                   'sections': sections, \n",
    "                   'labels': labels,\n",
    "                   'boundaries': boundaries}\n",
    "            \n",
    "    def get_generator(self):\n",
    "        return self._get_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e71a490-45e5-431b-b083-cd333771eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_func(input_text):    \n",
    "    \n",
    "    clean_text = re.sub(r'<[^<]+?>', '', input_text)\n",
    "    clean_text = re.sub(r'http\\S+', '', clean_text)\n",
    "    clean_text = clean_text.lower()\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    clean_text = unicodedata.normalize('NFKD', clean_text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    clean_text = contractions.fix(clean_text)\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', '', clean_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(clean_text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    clean_text = ' '.join(tokens)\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_words\n",
    "\n",
    "def create_dictionary(corpus):\n",
    "    word_freq = defaultdict(int)\n",
    "    for document in tqdm(corpus):\n",
    "        for token in document:\n",
    "            if len(token) > 2:\n",
    "                word_freq[token] += 1\n",
    "\n",
    "    all_words_set = set(words.words())\n",
    "    word_freq = {word: count for word, count in word_freq.items() if word in all_words_set}        \n",
    "    filtered_counts = {word: count for word, count in word_freq.items() if count > 30}\n",
    "    sorted_words = sorted(filtered_counts.items(), key=lambda x: x[0])\n",
    "    dictionary = {word: (index + 1) for index, (word, _) in enumerate(sorted_words)}\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def convert_to_uci(corpus_tockenized, dictionary, collection_name='my_collection'):\n",
    "    \n",
    "    # creating vocab file (all words have default class)\n",
    "    sorted_words = sorted(dictionary.items(), key=lambda x: x[0])\n",
    "    \n",
    "    with open(f'vocab.{collection_name}.txt', 'w') as file:\n",
    "        for token, _ in sorted_words:\n",
    "            file.write(f\"{token}\\n\")\n",
    "\n",
    "    D = len(corpus_tockenized)\n",
    "    W = len(dictionary)\n",
    "    output = []\n",
    "    for docid, doc in enumerate(corpus_tockenized):\n",
    "        word_freq = defaultdict(int)\n",
    "        for word in doc:\n",
    "            if word not in dictionary:\n",
    "                continue\n",
    "            word_freq[word] += 1\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: x[0])\n",
    "        for token, freq in sorted_words:\n",
    "            output.append([docid+1, dictionary[token], freq])\n",
    "            \n",
    "    NNZ = len(output)\n",
    "    \n",
    "    with open(f'docword.{collection_name}.txt', 'w') as file:\n",
    "        file.write(str(D) + '\\n')\n",
    "        file.write(str(W) + '\\n')\n",
    "        file.write(str(NNZ) + '\\n')\n",
    "\n",
    "        for doc_id, word_id, count in output:\n",
    "            line = f\"{doc_id} {word_id} {count}\"\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "def process_corpus(corpus):\n",
    "    tockenized_corpus = []\n",
    "    for document in tqdm(corpus):\n",
    "        cleaned_document = clean_text_func(document)\n",
    "        tockenized = lemmatize_text(cleaned_document)\n",
    "        tockenized_corpus.append(tockenized)\n",
    "    return tockenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "613bcc73-5476-48c9-b188-267f9ec8f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uci_to_vowpal(dict_path, file_path, output_path):\n",
    "    ix_to_word = dict()\n",
    "    counter = 1\n",
    "    with open(dict_path, 'r') as file:\n",
    "        for word in file.readlines():\n",
    "            ix_to_word[counter] = word.strip()\n",
    "            counter += 1\n",
    "            \n",
    "    new_data = defaultdict(list)\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            if len(line.strip().split(' ')) < 3:\n",
    "                continue\n",
    "            doc_id, word_ix, counter = list(map(int, line.strip().split(' ')))\n",
    "            if counter == 1:\n",
    "                new_data[doc_id].append(ix_to_word[word_ix])\n",
    "            else:\n",
    "                new_data[doc_id].append(f'{ix_to_word[word_ix]}:{counter}')\n",
    "    with open(output_path, 'w') as file:\n",
    "        for key in new_data.keys():\n",
    "            word_string = ' '.join(new_data[key])\n",
    "            file.write(f'doc{key} {word_string}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812aefd1-f2af-46a3-a852-780477829335",
   "metadata": {},
   "source": [
    "# Block for creation of new dataset in artm format"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5794374-6f4f-4f93-984d-bf3d9f350352",
   "metadata": {},
   "source": [
    "generator = WikiDataset('/home/dparinov/wiki_727/train/').get_generator()\n",
    "ds = Dataset.from_generator(generator)\n",
    "train_data = []\n",
    "for ix in range(len(ds)):\n",
    "    train_data.append(''.join(ds[ix]['sections']))\n",
    "tockenized_corpus = process_corpus(train_data)\n",
    "\n",
    "words_dict = create_dictionary(tockenized_corpus)\n",
    "convert_to_uci(tockenized_corpus, words_dict, collection_name='train')\n",
    "\n",
    "\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path='',\n",
    "                                        data_format='bow_uci',\n",
    "                                        collection_name='train',\n",
    "                                        target_folder='train_batches')\n",
    "\n",
    "dictionary = artm.Dictionary()\n",
    "dictionary.gather(data_path='train_batches',\n",
    "                  vocab_file_path='vocab.train.txt')\n",
    "\n",
    "dictionary.save(dictionary_path='train_batches/my_dictionary')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97e0a925-2831-4225-b907-1e70db422365",
   "metadata": {},
   "source": [
    "# building dataset for test\n",
    "\n",
    "generator = WikiDataset('/home/dparinov/wiki_727/test/').get_generator()\n",
    "ds = Dataset.from_generator(generator)\n",
    "test_data = []\n",
    "for ix in range(len(ds)):\n",
    "    test_data.append(''.join(ds[ix]['sections']))\n",
    "tockenized_corpus = process_corpus(test_data)\n",
    "\n",
    "# in case if run not in continuous manner\n",
    "# words_dict = dict()\n",
    "# with open('vocab.train.txt', 'r') as file:\n",
    "#     for word in file.readlines():\n",
    "#         words_dict[word.strip()] = len(words_dict) + 1\n",
    "        \n",
    "convert_to_uci(tockenized_corpus, words_dict, collection_name='test')\n",
    "\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path='',\n",
    "                                        data_format='bow_uci',\n",
    "                                        collection_name='test',\n",
    "                                        target_folder='test_batches')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5cbc50f-5084-4146-b0b0-e3b4cb4eee16",
   "metadata": {},
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path='train_batches',\n",
    "                                        data_format='batches')\n",
    "my_dictionary = artm.Dictionary()\n",
    "my_dictionary.load(dictionary_path='train_batches/my_dictionary.dict')\n",
    "\n",
    "model = artm.ARTM(num_topics=25, dictionary=my_dictionary)\n",
    "model.num_tokens = 10\n",
    "model.scores.add(artm.PerplexityScore(name='perplexity_score',\n",
    "                                      dictionary=my_dictionary))\n",
    "model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))\n",
    "model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))\n",
    "model.scores.add(artm.TopTokensScore(name='top_tokens_score'))\n",
    "\n",
    "model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='sparse_phi_regularizer'))\n",
    "model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='sparse_theta_regularizer'))\n",
    "model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer'))\n",
    "model.regularizers['sparse_phi_regularizer'].tau = -1 # 1e-5 for specific topics\n",
    "model.regularizers['sparse_theta_regularizer'].tau = -0.5\n",
    "model.regularizers['decorrelator_phi_regularizer'].tau = 1e5\n",
    "\n",
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=50)\n",
    "\n",
    "print(model.score_tracker['sparsity_phi_score'].value)\n",
    "print('**********************************************')\n",
    "\n",
    "saved_top_tokens = model.score_tracker['top_tokens_score'].last_tokens\n",
    "\n",
    "for topic_name in model.topic_names:\n",
    "     print(saved_top_tokens[topic_name])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b5951b4-aaf1-4a22-97b0-ebedb3e2b28c",
   "metadata": {},
   "source": [
    "with open('top_tokens.pickle', 'wb') as file:\n",
    "    pickle.dump(saved_top_tokens, file)\n",
    "    \n",
    "model.dump_artm_model('model_dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c091cd0-c989-4e0f-86e2-5c139061aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path='test_segments_batches',\n",
    "                                        data_format='batches')\n",
    "my_dictionary = artm.Dictionary()\n",
    "my_dictionary.load(dictionary_path='train_batches/my_dictionary.dict')\n",
    "\n",
    "\n",
    "with open('top_tokens.pickle', 'rb') as file:\n",
    "    topic_tokens = pickle.load(file)\n",
    "topics_qty = len(topic_tokens)\n",
    "model = artm.ARTM(num_topics=topics_qty, dictionary=my_dictionary)\n",
    "model.load('model_dump/n_wt.bin')\n",
    "model.load('model_dump/p_wt.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424cc2ec-7995-4b33-b6c2-e9d890bcd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_probs = model.transform(batch_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c11c7d-f9a8-4d66-a6f1-bf619631d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = topic_probs.columns.values.tolist()\n",
    "columns = sorted(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "464126f0-d51c-4ca5-a5d6-6ddde5ca5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_set = set(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f31be9bc-3f15-4462-9fa5-1bf03a68adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "for col in range(1, columns[-1] + 1):\n",
    "    if col in columns_set:\n",
    "        predicts.append(topic_probs[col].to_numpy())\n",
    "    else:\n",
    "        predicts.append(predicts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b84f16a-fd33-47f3-94a2-051ef8fec7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_numpy = np.array(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff144bce-f3cf-4862-9abf-c89e644115b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/home/dparinov/.cache/huggingface/datasets/generator/default-e076cdd982e0608b/0.0.0)\n",
      "100%|███████████████████████████████████| 73232/73232 [00:11<00:00, 6575.84it/s]\n"
     ]
    }
   ],
   "source": [
    "generator = WikiDataset('/home/dparinov/wiki_727/test/').get_generator()\n",
    "ds = Dataset.from_generator(generator)\n",
    "predicts_dict = dict()\n",
    "start = 0\n",
    "for id, doc in tqdm(enumerate(ds), total=len(ds)):\n",
    "    shift = len(doc['boundaries'])\n",
    "    predicts_dict[id] = predicts_numpy[start:start+shift]\n",
    "    start += shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ff6cb95-5e8d-4093-b5e3-b014fd9db5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predicts.pickle', 'wb') as file:\n",
    "    pickle.dump(predicts_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "792b26a4-3bcd-4266-9c07-e56a16927627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "af150ab5-66e8-4d8e-b64a-a7865b7cf04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_window(input, k=7):\n",
    "    output = np.zeros(input.shape)\n",
    "    for ix in range(input.shape[0]):\n",
    "        if ix < k:\n",
    "            output[ix] = input[:ix+1].sum(axis=0) / (ix + 1)\n",
    "        else:\n",
    "            output[ix] = output[ix - 1] - input[ix - k] / k + input[ix] / k\n",
    "    return output\n",
    "\n",
    "def right_window(input, k=7):\n",
    "    output = np.zeros(input.shape)\n",
    "    \n",
    "    for ix in range(input.shape[0]):\n",
    "        if ix == 0:\n",
    "            output[ix] = input[:k].sum(axis=0) / k\n",
    "        elif ix <= input.shape[0] - k:\n",
    "            output[ix] = output[ix - 1] - input[ix - 1] / k + input[ix + k - 1] / k\n",
    "        else:\n",
    "            output[ix] = input[ix:].sum(axis=0) / (input.shape[0] - ix)\n",
    "    return output\n",
    "\n",
    "def select_local_maxima(scores, threshold):\n",
    "    local_maxima = ['0'] * len(scores)\n",
    "    for ix in range(1, len(scores)):\n",
    "        if scores[ix] < threshold:\n",
    "            continue\n",
    "            \n",
    "        if ix == len(scores) - 1 and scores[ix] > scores[ix-1]:\n",
    "            local_maxima[ix] = '1'\n",
    "        elif scores[ix] > scores[ix-1] and scores[ix] > scores[ix+1]:\n",
    "            local_maxima[ix] = '1'\n",
    "\n",
    "    return ''.join(local_maxima)\n",
    "\n",
    "def boundary_score(input, threshold=None):\n",
    "    scores = []\n",
    "    for ix in range(len(input)):\n",
    "        l_max = input[ix]\n",
    "        l_ix = ix - 1\n",
    "        while l_ix >= 0:\n",
    "            if input[l_ix] > l_max:\n",
    "                l_max = input[l_ix]\n",
    "                l_ix -= 1\n",
    "            else:\n",
    "                break\n",
    "        r_max = input[ix]\n",
    "        r_ix = ix + 1\n",
    "        while r_ix < len(input):\n",
    "            if input[r_ix] > r_max:\n",
    "                r_max = input[r_ix]\n",
    "                r_ix += 1\n",
    "            else:\n",
    "                break\n",
    "        score = 0.5 * (l_max + r_max) - input[ix]\n",
    "        scores.append(score)\n",
    "    if threshold is None:\n",
    "        threshold = np.mean(scores) + 1 * np.std(scores)\n",
    "        \n",
    "    return select_local_maxima(scores, threshold)\n",
    "\n",
    "\n",
    "def get_similarity(lw, rw):\n",
    "    values = [1 - spatial.distance.cosine(a, b) for a, b in zip (lw, rw)]\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6c13de80-e2bd-4627-ac99-44a00ecfbf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/home/dparinov/.cache/huggingface/datasets/generator/default-e076cdd982e0608b/0.0.0)\n",
      "100%|████████████████████████████████████| 73232/73232 [02:55<00:00, 416.25it/s]\n"
     ]
    }
   ],
   "source": [
    "generator = WikiDataset('/home/dparinov/wiki_727/test/').get_generator()\n",
    "ds = Dataset.from_generator(generator)\n",
    "pk = []\n",
    "wd = []\n",
    "counter = 1\n",
    "\n",
    "for id, doc in tqdm(enumerate(ds), total=len(ds)):\n",
    "    ref = doc['boundaries']\n",
    "    lw = left_window(predicts_dict[id])\n",
    "    rw = right_window(predicts_dict[id])\n",
    "    scores = get_similarity(lw, rw)\n",
    "    preds = boundary_score(scores)\n",
    "    pk.append(nltk.pk(ref, preds, k=5))\n",
    "    wd.append(nltk.windowdiff(ref, preds, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "66a0bdd7-c313-4f8e-a845-d67d2dec85ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4355168432478926"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pk) / len(pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5f40a4bc-870a-4e69-8666-b6906e652117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4938090940510369"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(wd) / len(wd)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fe0a31b-df4d-4ead-b852-adde6e60a7ab",
   "metadata": {},
   "source": [
    "-1std 0.4359771905968345 0.5938036728868297\n",
    "-0.5std 0.4312677309906048 0.576870666013157\n",
    "0std 0.42490328755054196 0.5271623905657693\n",
    "0.5std 0.4275348112140699 0.5038027074282835\n",
    "1std 0.4355168432478926 0.4938090940510369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e6d63-ebe0-4dbf-bc2a-3b1f6b19f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tqdm(ds, total=len(ds)):\n",
    "    doc_dict = dict()\n",
    "    doc_dict['real'] = doc['boundaries']\n",
    "    vectors = []\n",
    "    for sentence in doc['sections']:\n",
    "        sentence_cleaned = clean_text_func(sentence)\n",
    "        tockens = lemmatize_text(sentence_cleaned)\n",
    "        vector = vectorizer.transform([' '.join(tockens)])\n",
    "        vectors.append(vector.toarray()[0])\n",
    "    batch_vectorizer = artm.BatchVectorizer(data_format='bow_n_wd',\n",
    "                                    n_wd=np.array(vectors).T,\n",
    "                                    vocabulary=words_dict_artm)\n",
    "    topic_probs = model.transform(batch_vectorizer)\n",
    "    predicts = topic_probs.to_numpy().argmax(axis=0).tolist()\n",
    "    predicts_borders = [0]\n",
    "    for ix in range(1, len(predicts)):\n",
    "        if predicts[ix] != predicts[ix-1]:\n",
    "            predicts_borders.append(1)\n",
    "        else:\n",
    "            predicts_borders.append(0)\n",
    "    doc_dict['predict'] = predicts_borders\n",
    "    break\n",
    "    result.append(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd045c-439e-42e8-89ed-65107f742ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0cfb3-5b14-4abf-9b7e-1a3f991ac128",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = []\n",
    "pk = []\n",
    "for example in result:\n",
    "    test = example['real']\n",
    "    predict = ''.join(list(map(str, example['predict'])))\n",
    "    pk.append(nltk.pk(test, predict, k=5))\n",
    "    wd.append(nltk.windowdiff(test, predict, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a43e73-fdfc-41bc-a78b-1bcc00dd2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pk) / len(pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b732941-828c-44ee-80f9-a550189c76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c2ad8-4efa-4a98-ab26-a617d7be78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(wd) / len(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb81c93-5772-4722-9104-c7652afd82ec",
   "metadata": {},
   "source": [
    "# Block for loading model and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9476f2-381b-4c78-a301-8c1ac51f3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_dict = artm.Dictionary()\n",
    "cooc_dict.gather(\n",
    "    data_path='test_batches',\n",
    "    cooc_file_path='cooc_df_test',\n",
    "    vocab_file_path='vocab.train.txt',\n",
    "    symmetric_cooc_values=True)\n",
    "\n",
    "with open('top_tokens.pickle', 'rb') as file:\n",
    "    topic_tokens = pickle.load(file)\n",
    "topics_qty = len(topic_tokens)\n",
    "model = artm.ARTM(num_topics=topics_qty, dictionary=cooc_dict)\n",
    "model.load('model_dump/n_wt.bin')\n",
    "model.load('model_dump/p_wt.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46d28c-e05a-4a98-b7f1-847758830845",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_score = artm.TopTokensScore(\n",
    "                            name='TopTokensCoherenceScore',\n",
    "                            class_id='@default_class',\n",
    "                            num_tokens=12,\n",
    "                            dictionary=cooc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff13f9a-339b-4892-8a94-27f3739b1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.scores.add(coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879a75b-1fe8-4559-b357-14899ebe8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path='test_batches',\n",
    "                                        data_format='batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2e8f6-3d5a-443e-b753-aac74dc2c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_cd_test = model.transform(batch_vectorizer=batch_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a5619-4e25-454a-ae8c-0254b7bff790",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_cd_test.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1b1b5-85fa-499d-a89d-67236c3a2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not work for some reason, will add manual calculation\n",
    "model.score_tracker['TopTokensCoherenceScore'].coherence[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86572a0-87b5-45f7-ba63-c5f0e97c46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice -n 5 bigartm -c vw_test.txt -v vocab.train.txt --cooc-window 10 --cooc-min-tf 5 --cooc-min-df 5 --write-cooc-tf cooc_tf_test --write-cooc-df cooc_df_test --write-ppmi-tf ppmi_tf_test --write-ppmi-df ppmi_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94271cb-5d6f-4c43-81b4-315ed556194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top_tokens.pickle', 'rb') as file:\n",
    "    top_tockens = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80602d2-e588-4a2c-abd2-e058e7c852b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_coherence(ppmi_file, topic_tockens):\n",
    "    cooc_dict = dict()\n",
    "    with open(ppmi_file, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            word, *counts = line.strip().split(' ')\n",
    "            if word not in cooc_dict:\n",
    "                cooc_dict[word] = dict()\n",
    "            else:\n",
    "                print('something went wrong')\n",
    "            for counter in counts:\n",
    "                word_c = counter.split(':')\n",
    "                cooc_dict[word][word_c[0]] = float(word_c[-1])\n",
    "    result = dict()\n",
    "    for topic_name in topic_tockens.keys():\n",
    "        tockens = topic_tockens[topic_name]\n",
    "        sum = 0\n",
    "        for i in range(len(tockens) - 1):\n",
    "            for j in range(i + 1, len(tockens)):\n",
    "                sum += cooc_dict.get(tockens[i], dict()).get(tockens[j], 0)\n",
    "        result[topic_name] = sum * 2 / (len(tockens) * (len(tockens) - 1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5602e74-c877-418a-94aa-d9b140211b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calc_coherence('ppmi_df_test', top_tockens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2719906-5501-472c-a093-30f670e25ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list(res.values())) / 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432a129-e4f1-4379-a2cb-6d3fc9f27c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
